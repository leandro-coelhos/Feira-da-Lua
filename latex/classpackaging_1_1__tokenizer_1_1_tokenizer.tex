\doxysection{packaging.\+\_\+tokenizer.\+Tokenizer Class Reference}
\hypertarget{classpackaging_1_1__tokenizer_1_1_tokenizer}{}\label{classpackaging_1_1__tokenizer_1_1_tokenizer}\index{packaging.\_tokenizer.Tokenizer@{packaging.\_tokenizer.Tokenizer}}
\doxysubsubsection*{Public Member Functions}
\begin{DoxyCompactItemize}
\item 
None \mbox{\hyperlink{classpackaging_1_1__tokenizer_1_1_tokenizer_a0c469483e353b7ad4ec75d439a0716f7}{\+\_\+\+\_\+init\+\_\+\+\_\+}} (self, str \mbox{\hyperlink{classpackaging_1_1__tokenizer_1_1_tokenizer_ac59ba1bb77d052cf509b55868fd8a8ac}{source}}, \texorpdfstring{$\ast$}{*}, dict\mbox{[}str, str\texorpdfstring{$\vert$}{|}re.\+Pattern\mbox{[}str\mbox{]}\mbox{]} \mbox{\hyperlink{classpackaging_1_1__tokenizer_1_1_tokenizer_a37276fa7c417730f020c9838b9ad431c}{rules}})
\item 
None \mbox{\hyperlink{classpackaging_1_1__tokenizer_1_1_tokenizer_aff74eaccaae690982aaaba5cb9a4d051}{consume}} (self, str name)
\item 
bool \mbox{\hyperlink{classpackaging_1_1__tokenizer_1_1_tokenizer_a6c4529c0615ee80330bf28db87789ab6}{check}} (self, str name, \texorpdfstring{$\ast$}{*}, bool peek=False)
\item 
\mbox{\hyperlink{classpackaging_1_1__tokenizer_1_1_token}{Token}} \mbox{\hyperlink{classpackaging_1_1__tokenizer_1_1_tokenizer_a151b21b99f387eddd422cd0dcf2064dd}{expect}} (self, str name, \texorpdfstring{$\ast$}{*}, str expected)
\item 
\mbox{\hyperlink{classpackaging_1_1__tokenizer_1_1_token}{Token}} \mbox{\hyperlink{classpackaging_1_1__tokenizer_1_1_tokenizer_a0338e1186dd1574082f047fdff9b3b2d}{read}} (self)
\item 
No\+Return \mbox{\hyperlink{classpackaging_1_1__tokenizer_1_1_tokenizer_ada56238a9429c16510f207c95f22645f}{raise\+\_\+syntax\+\_\+error}} (self, str message, \texorpdfstring{$\ast$}{*}, int\texorpdfstring{$\vert$}{|}None span\+\_\+start=None, int\texorpdfstring{$\vert$}{|}None span\+\_\+end=None)
\item 
Iterator\mbox{[}None\mbox{]} \mbox{\hyperlink{classpackaging_1_1__tokenizer_1_1_tokenizer_a27f8cfa7a5ed91e0cba1923b9b18da3d}{enclosing\+\_\+tokens}} (self, str open\+\_\+token, str close\+\_\+token, \texorpdfstring{$\ast$}{*}, str around)
\end{DoxyCompactItemize}
\doxysubsubsection*{Public Attributes}
\begin{DoxyCompactItemize}
\item 
\mbox{\hyperlink{classpackaging_1_1__tokenizer_1_1_tokenizer_ac59ba1bb77d052cf509b55868fd8a8ac}{source}} = source
\item 
dict \mbox{\hyperlink{classpackaging_1_1__tokenizer_1_1_tokenizer_a37276fa7c417730f020c9838b9ad431c}{rules}}
\item 
\mbox{\hyperlink{classpackaging_1_1__tokenizer_1_1_token}{Token}}\texorpdfstring{$\vert$}{|}None \mbox{\hyperlink{classpackaging_1_1__tokenizer_1_1_tokenizer_a7ab04f8b5294885f9420b83231982240}{next\+\_\+token}} = None
\item 
int \mbox{\hyperlink{classpackaging_1_1__tokenizer_1_1_tokenizer_a1ce175ba4f8e8fa56c5d564bccf80e83}{position}} = 0
\end{DoxyCompactItemize}


\doxysubsection{Detailed Description}
\begin{DoxyVerb}Context-sensitive token parsing.

Provides methods to examine the input stream to check whether the next token
matches.
\end{DoxyVerb}
 

\label{doc-constructors}
\Hypertarget{classpackaging_1_1__tokenizer_1_1_tokenizer_doc-constructors}
\doxysubsection{Constructor \& Destructor Documentation}
\Hypertarget{classpackaging_1_1__tokenizer_1_1_tokenizer_a0c469483e353b7ad4ec75d439a0716f7}\index{packaging.\_tokenizer.Tokenizer@{packaging.\_tokenizer.Tokenizer}!\_\_init\_\_@{\_\_init\_\_}}
\index{\_\_init\_\_@{\_\_init\_\_}!packaging.\_tokenizer.Tokenizer@{packaging.\_tokenizer.Tokenizer}}
\doxysubsubsection{\texorpdfstring{\_\_init\_\_()}{\_\_init\_\_()}}
{\footnotesize\ttfamily \label{classpackaging_1_1__tokenizer_1_1_tokenizer_a0c469483e353b7ad4ec75d439a0716f7} 
 None packaging.\+\_\+tokenizer.\+Tokenizer.\+\_\+\+\_\+init\+\_\+\+\_\+ (\begin{DoxyParamCaption}\item[{}]{self}{, }\item[{str}]{source}{, }\item[{\texorpdfstring{$\ast$}{*}}]{}{, }\item[{dict\mbox{[}str, str \texorpdfstring{$\vert$}{|} re.\+Pattern\mbox{[}str\mbox{]}\mbox{]}}]{rules}{}\end{DoxyParamCaption})}



\label{doc-func-members}
\Hypertarget{classpackaging_1_1__tokenizer_1_1_tokenizer_doc-func-members}
\doxysubsection{Member Function Documentation}
\Hypertarget{classpackaging_1_1__tokenizer_1_1_tokenizer_a6c4529c0615ee80330bf28db87789ab6}\index{packaging.\_tokenizer.Tokenizer@{packaging.\_tokenizer.Tokenizer}!check@{check}}
\index{check@{check}!packaging.\_tokenizer.Tokenizer@{packaging.\_tokenizer.Tokenizer}}
\doxysubsubsection{\texorpdfstring{check()}{check()}}
{\footnotesize\ttfamily \label{classpackaging_1_1__tokenizer_1_1_tokenizer_a6c4529c0615ee80330bf28db87789ab6} 
 bool packaging.\+\_\+tokenizer.\+Tokenizer.\+check (\begin{DoxyParamCaption}\item[{}]{self}{, }\item[{str}]{name}{, }\item[{\texorpdfstring{$\ast$}{*}}]{}{, }\item[{bool }]{peek}{ = {\ttfamily False}}\end{DoxyParamCaption})}

\begin{DoxyVerb}Check whether the next token has the provided name.

By default, if the check succeeds, the token *must* be read before
another check. If `peek` is set to `True`, the token is not loaded and
would need to be checked again.
\end{DoxyVerb}
 \Hypertarget{classpackaging_1_1__tokenizer_1_1_tokenizer_aff74eaccaae690982aaaba5cb9a4d051}\index{packaging.\_tokenizer.Tokenizer@{packaging.\_tokenizer.Tokenizer}!consume@{consume}}
\index{consume@{consume}!packaging.\_tokenizer.Tokenizer@{packaging.\_tokenizer.Tokenizer}}
\doxysubsubsection{\texorpdfstring{consume()}{consume()}}
{\footnotesize\ttfamily \label{classpackaging_1_1__tokenizer_1_1_tokenizer_aff74eaccaae690982aaaba5cb9a4d051} 
 None packaging.\+\_\+tokenizer.\+Tokenizer.\+consume (\begin{DoxyParamCaption}\item[{}]{self}{, }\item[{str}]{name}{}\end{DoxyParamCaption})}

\begin{DoxyVerb}Move beyond provided token name, if at current position.\end{DoxyVerb}
 \Hypertarget{classpackaging_1_1__tokenizer_1_1_tokenizer_a27f8cfa7a5ed91e0cba1923b9b18da3d}\index{packaging.\_tokenizer.Tokenizer@{packaging.\_tokenizer.Tokenizer}!enclosing\_tokens@{enclosing\_tokens}}
\index{enclosing\_tokens@{enclosing\_tokens}!packaging.\_tokenizer.Tokenizer@{packaging.\_tokenizer.Tokenizer}}
\doxysubsubsection{\texorpdfstring{enclosing\_tokens()}{enclosing\_tokens()}}
{\footnotesize\ttfamily \label{classpackaging_1_1__tokenizer_1_1_tokenizer_a27f8cfa7a5ed91e0cba1923b9b18da3d} 
 Iterator\mbox{[}None\mbox{]} packaging.\+\_\+tokenizer.\+Tokenizer.\+enclosing\+\_\+tokens (\begin{DoxyParamCaption}\item[{}]{self}{, }\item[{str}]{open\+\_\+token}{, }\item[{str}]{close\+\_\+token}{, }\item[{\texorpdfstring{$\ast$}{*}}]{}{, }\item[{str     }]{around}{}\end{DoxyParamCaption})}

\Hypertarget{classpackaging_1_1__tokenizer_1_1_tokenizer_a151b21b99f387eddd422cd0dcf2064dd}\index{packaging.\_tokenizer.Tokenizer@{packaging.\_tokenizer.Tokenizer}!expect@{expect}}
\index{expect@{expect}!packaging.\_tokenizer.Tokenizer@{packaging.\_tokenizer.Tokenizer}}
\doxysubsubsection{\texorpdfstring{expect()}{expect()}}
{\footnotesize\ttfamily \label{classpackaging_1_1__tokenizer_1_1_tokenizer_a151b21b99f387eddd422cd0dcf2064dd} 
 \mbox{\hyperlink{classpackaging_1_1__tokenizer_1_1_token}{Token}} packaging.\+\_\+tokenizer.\+Tokenizer.\+expect (\begin{DoxyParamCaption}\item[{}]{self}{, }\item[{str}]{name}{, }\item[{\texorpdfstring{$\ast$}{*}}]{}{, }\item[{str}]{expected}{}\end{DoxyParamCaption})}

\begin{DoxyVerb}Expect a certain token name next, failing with a syntax error otherwise.

The token is *not* read.
\end{DoxyVerb}
 \Hypertarget{classpackaging_1_1__tokenizer_1_1_tokenizer_ada56238a9429c16510f207c95f22645f}\index{packaging.\_tokenizer.Tokenizer@{packaging.\_tokenizer.Tokenizer}!raise\_syntax\_error@{raise\_syntax\_error}}
\index{raise\_syntax\_error@{raise\_syntax\_error}!packaging.\_tokenizer.Tokenizer@{packaging.\_tokenizer.Tokenizer}}
\doxysubsubsection{\texorpdfstring{raise\_syntax\_error()}{raise\_syntax\_error()}}
{\footnotesize\ttfamily \label{classpackaging_1_1__tokenizer_1_1_tokenizer_ada56238a9429c16510f207c95f22645f} 
 No\+Return packaging.\+\_\+tokenizer.\+Tokenizer.\+raise\+\_\+syntax\+\_\+error (\begin{DoxyParamCaption}\item[{}]{self}{, }\item[{str}]{message}{, }\item[{\texorpdfstring{$\ast$}{*}}]{}{, }\item[{int \texorpdfstring{$\vert$}{|} None }]{span\+\_\+start}{ = {\ttfamily None}, }\item[{int \texorpdfstring{$\vert$}{|} None }]{span\+\_\+end}{ = {\ttfamily None}}\end{DoxyParamCaption})}

\begin{DoxyVerb}Raise ParserSyntaxError at the given position.\end{DoxyVerb}
 \Hypertarget{classpackaging_1_1__tokenizer_1_1_tokenizer_a0338e1186dd1574082f047fdff9b3b2d}\index{packaging.\_tokenizer.Tokenizer@{packaging.\_tokenizer.Tokenizer}!read@{read}}
\index{read@{read}!packaging.\_tokenizer.Tokenizer@{packaging.\_tokenizer.Tokenizer}}
\doxysubsubsection{\texorpdfstring{read()}{read()}}
{\footnotesize\ttfamily \label{classpackaging_1_1__tokenizer_1_1_tokenizer_a0338e1186dd1574082f047fdff9b3b2d} 
 \mbox{\hyperlink{classpackaging_1_1__tokenizer_1_1_token}{Token}} packaging.\+\_\+tokenizer.\+Tokenizer.\+read (\begin{DoxyParamCaption}\item[{}]{self}{}\end{DoxyParamCaption})}

\begin{DoxyVerb}Consume the next token and return it.\end{DoxyVerb}
 

\label{doc-variable-members}
\Hypertarget{classpackaging_1_1__tokenizer_1_1_tokenizer_doc-variable-members}
\doxysubsection{Member Data Documentation}
\Hypertarget{classpackaging_1_1__tokenizer_1_1_tokenizer_a7ab04f8b5294885f9420b83231982240}\index{packaging.\_tokenizer.Tokenizer@{packaging.\_tokenizer.Tokenizer}!next\_token@{next\_token}}
\index{next\_token@{next\_token}!packaging.\_tokenizer.Tokenizer@{packaging.\_tokenizer.Tokenizer}}
\doxysubsubsection{\texorpdfstring{next\_token}{next\_token}}
{\footnotesize\ttfamily \label{classpackaging_1_1__tokenizer_1_1_tokenizer_a7ab04f8b5294885f9420b83231982240} 
\mbox{\hyperlink{classpackaging_1_1__tokenizer_1_1_token}{Token}} \texorpdfstring{$\vert$}{|} None packaging.\+\_\+tokenizer.\+Tokenizer.\+next\+\_\+token = None}

\Hypertarget{classpackaging_1_1__tokenizer_1_1_tokenizer_a1ce175ba4f8e8fa56c5d564bccf80e83}\index{packaging.\_tokenizer.Tokenizer@{packaging.\_tokenizer.Tokenizer}!position@{position}}
\index{position@{position}!packaging.\_tokenizer.Tokenizer@{packaging.\_tokenizer.Tokenizer}}
\doxysubsubsection{\texorpdfstring{position}{position}}
{\footnotesize\ttfamily \label{classpackaging_1_1__tokenizer_1_1_tokenizer_a1ce175ba4f8e8fa56c5d564bccf80e83} 
int packaging.\+\_\+tokenizer.\+Tokenizer.\+position = 0}

\Hypertarget{classpackaging_1_1__tokenizer_1_1_tokenizer_a37276fa7c417730f020c9838b9ad431c}\index{packaging.\_tokenizer.Tokenizer@{packaging.\_tokenizer.Tokenizer}!rules@{rules}}
\index{rules@{rules}!packaging.\_tokenizer.Tokenizer@{packaging.\_tokenizer.Tokenizer}}
\doxysubsubsection{\texorpdfstring{rules}{rules}}
{\footnotesize\ttfamily \label{classpackaging_1_1__tokenizer_1_1_tokenizer_a37276fa7c417730f020c9838b9ad431c} 
packaging.\+\_\+tokenizer.\+Tokenizer.\+rules}

{\bfseries Initial value\+:}
\begin{DoxyCode}{0}
\DoxyCodeLine{=\ \ \{}
\DoxyCodeLine{\ \ \ \ \ \ \ \ \ \ \ \ name:\ re.compile(pattern)\ \textcolor{keywordflow}{for}\ name,\ pattern\ \textcolor{keywordflow}{in}\ rules.items()}
\DoxyCodeLine{\ \ \ \ \ \ \ \ \}}

\end{DoxyCode}
\Hypertarget{classpackaging_1_1__tokenizer_1_1_tokenizer_ac59ba1bb77d052cf509b55868fd8a8ac}\index{packaging.\_tokenizer.Tokenizer@{packaging.\_tokenizer.Tokenizer}!source@{source}}
\index{source@{source}!packaging.\_tokenizer.Tokenizer@{packaging.\_tokenizer.Tokenizer}}
\doxysubsubsection{\texorpdfstring{source}{source}}
{\footnotesize\ttfamily \label{classpackaging_1_1__tokenizer_1_1_tokenizer_ac59ba1bb77d052cf509b55868fd8a8ac} 
packaging.\+\_\+tokenizer.\+Tokenizer.\+source = source}



The documentation for this class was generated from the following file\+:\begin{DoxyCompactItemize}
\item 
feira\+\_\+venv/\+Lib/site-\/packages/packaging/\mbox{\hyperlink{packaging_2__tokenizer_8py}{\+\_\+tokenizer.\+py}}\end{DoxyCompactItemize}
